{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2022-05-09T19:26:20.723214Z","iopub.status.busy":"2022-05-09T19:26:20.722861Z","iopub.status.idle":"2022-05-09T19:26:20.728277Z","shell.execute_reply":"2022-05-09T19:26:20.726871Z","shell.execute_reply.started":"2022-05-09T19:26:20.723178Z"},"trusted":true},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","import os\n","import matplotlib.pyplot as plt\n","import torch\n","from torch.utils.data import Dataset, DataLoader\n","from torchvision import datasets\n","from torchvision import transforms\n","from torchvision.io import read_image\n","from pathlib import Path"]},{"cell_type":"markdown","metadata":{},"source":["https://towardsdatascience.com/implementing-visualttransformer-in-pytorch-184f9f16f632"]},{"cell_type":"code","execution_count":16,"metadata":{"execution":{"iopub.execute_input":"2022-05-09T18:13:10.28485Z","iopub.status.busy":"2022-05-09T18:13:10.284572Z","iopub.status.idle":"2022-05-09T18:13:10.470853Z","shell.execute_reply":"2022-05-09T18:13:10.469611Z","shell.execute_reply.started":"2022-05-09T18:13:10.28482Z"},"trusted":true},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>primary_label</th>\n","      <th>secondary_labels</th>\n","      <th>type</th>\n","      <th>latitude</th>\n","      <th>longitude</th>\n","      <th>scientific_name</th>\n","      <th>common_name</th>\n","      <th>author</th>\n","      <th>license</th>\n","      <th>rating</th>\n","      <th>time</th>\n","      <th>url</th>\n","      <th>filename</th>\n","      <th>file_path</th>\n","      <th>kfold</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>afrsil1</td>\n","      <td>[]</td>\n","      <td>['call', 'flight call']</td>\n","      <td>12.3910</td>\n","      <td>-1.4930</td>\n","      <td>Euodice cantans</td>\n","      <td>African Silverbill</td>\n","      <td>Bram Piot</td>\n","      <td>Creative Commons Attribution-NonCommercial-Sha...</td>\n","      <td>2.5</td>\n","      <td>08:00</td>\n","      <td>https://www.xeno-canto.org/125458</td>\n","      <td>afrsil1/XC125458.ogg</td>\n","      <td>birdclef-2022/train_audio/afrsil1/XC125458.ogg</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>afrsil1</td>\n","      <td>['houspa', 'redava', 'zebdov']</td>\n","      <td>['call']</td>\n","      <td>19.8801</td>\n","      <td>-155.7254</td>\n","      <td>Euodice cantans</td>\n","      <td>African Silverbill</td>\n","      <td>Dan Lane</td>\n","      <td>Creative Commons Attribution-NonCommercial-Sha...</td>\n","      <td>3.5</td>\n","      <td>08:30</td>\n","      <td>https://www.xeno-canto.org/175522</td>\n","      <td>afrsil1/XC175522.ogg</td>\n","      <td>birdclef-2022/train_audio/afrsil1/XC175522.ogg</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>afrsil1</td>\n","      <td>[]</td>\n","      <td>['call', 'song']</td>\n","      <td>16.2901</td>\n","      <td>-16.0321</td>\n","      <td>Euodice cantans</td>\n","      <td>African Silverbill</td>\n","      <td>Bram Piot</td>\n","      <td>Creative Commons Attribution-NonCommercial-Sha...</td>\n","      <td>4.0</td>\n","      <td>11:30</td>\n","      <td>https://www.xeno-canto.org/177993</td>\n","      <td>afrsil1/XC177993.ogg</td>\n","      <td>birdclef-2022/train_audio/afrsil1/XC177993.ogg</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>afrsil1</td>\n","      <td>[]</td>\n","      <td>['alarm call', 'call']</td>\n","      <td>17.0922</td>\n","      <td>54.2958</td>\n","      <td>Euodice cantans</td>\n","      <td>African Silverbill</td>\n","      <td>Oscar Campbell</td>\n","      <td>Creative Commons Attribution-NonCommercial-Sha...</td>\n","      <td>4.0</td>\n","      <td>11:00</td>\n","      <td>https://www.xeno-canto.org/205893</td>\n","      <td>afrsil1/XC205893.ogg</td>\n","      <td>birdclef-2022/train_audio/afrsil1/XC205893.ogg</td>\n","      <td>4</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>afrsil1</td>\n","      <td>[]</td>\n","      <td>['flight call']</td>\n","      <td>21.4581</td>\n","      <td>-157.7252</td>\n","      <td>Euodice cantans</td>\n","      <td>African Silverbill</td>\n","      <td>Ross Gallardy</td>\n","      <td>Creative Commons Attribution-NonCommercial-Sha...</td>\n","      <td>3.0</td>\n","      <td>16:30</td>\n","      <td>https://www.xeno-canto.org/207431</td>\n","      <td>afrsil1/XC207431.ogg</td>\n","      <td>birdclef-2022/train_audio/afrsil1/XC207431.ogg</td>\n","      <td>4</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["  primary_label                secondary_labels                     type  \\\n","0       afrsil1                              []  ['call', 'flight call']   \n","1       afrsil1  ['houspa', 'redava', 'zebdov']                 ['call']   \n","2       afrsil1                              []         ['call', 'song']   \n","3       afrsil1                              []   ['alarm call', 'call']   \n","4       afrsil1                              []          ['flight call']   \n","\n","   latitude  longitude  scientific_name         common_name          author  \\\n","0   12.3910    -1.4930  Euodice cantans  African Silverbill       Bram Piot   \n","1   19.8801  -155.7254  Euodice cantans  African Silverbill        Dan Lane   \n","2   16.2901   -16.0321  Euodice cantans  African Silverbill       Bram Piot   \n","3   17.0922    54.2958  Euodice cantans  African Silverbill  Oscar Campbell   \n","4   21.4581  -157.7252  Euodice cantans  African Silverbill   Ross Gallardy   \n","\n","                                             license  rating   time  \\\n","0  Creative Commons Attribution-NonCommercial-Sha...     2.5  08:00   \n","1  Creative Commons Attribution-NonCommercial-Sha...     3.5  08:30   \n","2  Creative Commons Attribution-NonCommercial-Sha...     4.0  11:30   \n","3  Creative Commons Attribution-NonCommercial-Sha...     4.0  11:00   \n","4  Creative Commons Attribution-NonCommercial-Sha...     3.0  16:30   \n","\n","                                 url              filename  \\\n","0  https://www.xeno-canto.org/125458  afrsil1/XC125458.ogg   \n","1  https://www.xeno-canto.org/175522  afrsil1/XC175522.ogg   \n","2  https://www.xeno-canto.org/177993  afrsil1/XC177993.ogg   \n","3  https://www.xeno-canto.org/205893  afrsil1/XC205893.ogg   \n","4  https://www.xeno-canto.org/207431  afrsil1/XC207431.ogg   \n","\n","                                        file_path  kfold  \n","0  birdclef-2022/train_audio/afrsil1/XC125458.ogg      0  \n","1  birdclef-2022/train_audio/afrsil1/XC175522.ogg      0  \n","2  birdclef-2022/train_audio/afrsil1/XC177993.ogg      1  \n","3  birdclef-2022/train_audio/afrsil1/XC205893.ogg      4  \n","4  birdclef-2022/train_audio/afrsil1/XC207431.ogg      4  "]},"execution_count":16,"metadata":{},"output_type":"execute_result"}],"source":["# annotation_file = Path(\"./birdclef-2022/train_metadata.csv\")\n","annotation_file = Path(\"./train_folds.csv\")\n","\n","data_dir = Path(\"./birdclef-2022/spectograms/\")\n","\n","dataframe = pd.read_csv(annotation_file)\n","dataframe.head()"]},{"cell_type":"markdown","metadata":{},"source":["## loading data"]},{"cell_type":"code","execution_count":52,"metadata":{},"outputs":[],"source":["class CustomDataset(Dataset):\n","    def __init__(self, annotations_file, data_dir, transform=None, target_transform=None):\n","        self.labels = pd.read_csv(annotations_file)\n","        self.data_dir = data_dir\n","        self.transform = transform\n","        self.target_transform = target_transform\n","\n","    def __len__(self):\n","        return len(self.labels)\n","\n","    def __getitem__(self, idx):\n","        img_path = data_dir / (self.labels.iloc[idx].filename[:-4] + \".npy\")\n","        print(img_path)\n","        spect = np.load(img_path)\n","        label = self.labels.iloc[idx, 1]\n","        if self.transform:\n","            spect = self.transform(spect)\n","        if self.target_transform:\n","            label = self.target_transform(label)\n","        return spect, label"]},{"cell_type":"code","execution_count":53,"metadata":{},"outputs":[],"source":["dataset = CustomDataset(\n","    annotations_file = annotation_file,\n","    data_dir = data_dir,\n","    transform = transforms.ToTensor(),\n",")"]},{"cell_type":"code","execution_count":54,"metadata":{},"outputs":[{"data":{"text/plain":["(11881.6, 14852)"]},"execution_count":54,"metadata":{},"output_type":"execute_result"}],"source":["len(dataset) * 0.8, len(dataset)"]},{"cell_type":"code","execution_count":55,"metadata":{},"outputs":[],"source":["train_set, val_set = torch.utils.data.random_split(dataset, [11881, 2971], generator=torch.Generator().manual_seed(42))\n","train_dataloader = DataLoader(train_set, batch_size=1)\n","val_dataloader = DataLoader(val_set, batch_size=1)"]},{"cell_type":"code","execution_count":59,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["birdclef-2022/spectograms/cangoo/XC407057.npy\n","Feature batch shape: torch.Size([1, 3, 224, 313])\n"]},{"ename":"AttributeError","evalue":"'tuple' object has no attribute 'size'","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;32m<ipython-input-59-c0fe5bfd2416>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtrain_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Feature batch shape: {train_features.size()}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Labels batch shape: {train_labels.size()}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_features\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_labels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mAttributeError\u001b[0m: 'tuple' object has no attribute 'size'"]}],"source":["train_features, train_labels = next(iter(train_dataloader))\n","print(f\"Feature batch shape: {train_features.size()}\")\n","print(f\"Labels batch shape: {train_labels.size()}\")\n","img = train_features[0].squeeze()\n","label = train_labels[0]\n","plt.imshow(img.T.swapaxes(0,1), cmap=\"gray\")\n","plt.show()\n","print(f\"Label: {label}\")"]},{"cell_type":"markdown","metadata":{},"source":["## Model architecture"]},{"cell_type":"code","execution_count":27,"metadata":{},"outputs":[],"source":["import torch\n","import torch.nn.functional as F\n","\n","from torch import nn\n","from torch import Tensor\n","from PIL import Image\n","from torchvision.transforms import Compose, Resize, ToTensor\n","from einops import rearrange, reduce, repeat\n","from einops.layers.torch import Rearrange, Reduce"]},{"cell_type":"code","execution_count":30,"metadata":{},"outputs":[{"data":{"text/plain":["tensor([[8.9865e-03, 4.2636e-03, 5.1008e-05,  ..., 1.1000e-05, 2.0570e-03,\n","         8.0032e-03],\n","        [9.0097e-03, 4.2936e-03, 3.7561e-05,  ..., 1.2993e-05, 2.0744e-03,\n","         8.0402e-03],\n","        [9.0676e-03, 4.3042e-03, 5.1448e-05,  ..., 1.3919e-05, 2.1201e-03,\n","         8.0720e-03],\n","        ...,\n","        [2.1308e-03, 9.8159e-04, 6.5151e-05,  ..., 3.6590e-05, 1.5210e-04,\n","         5.9034e-04],\n","        [2.1655e-03, 1.0791e-03, 1.3939e-04,  ..., 2.4489e-05, 1.6248e-04,\n","         6.0643e-04],\n","        [2.2068e-03, 1.1434e-03, 6.5144e-05,  ..., 1.2710e-05, 2.1759e-04,\n","         6.4147e-04]])"]},"execution_count":30,"metadata":{},"output_type":"execute_result"}],"source":["X_train[0]"]},{"cell_type":"code","execution_count":29,"metadata":{},"outputs":[{"ename":"ValueError","evalue":"not enough values to unpack (expected 4, got 2)","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-29-be28af8930ee>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m \u001b[0mPatchEmbedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-29-be28af8930ee>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m     \u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mproj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0mcls_tokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrepeat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcls_token\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'() n e -> b n e'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: not enough values to unpack (expected 4, got 2)"]}],"source":["class PatchEmbedding(nn.Module):\n","  def __init__(self,\n","    in_channels: int = 1,\n","    patch_size: int = 16,\n","    emb_size: int = 768):\n","\n","    super().__init__()\n","    self.proj = nn.Sequential(\n","      nn.Conv2d(in_channels,\n","        emb_size,\n","        kernel_size = patch_size,\n","        stride = patch_size),\n","      Rearrange('b e (h) (w) -> b (h w) e'),\n","    )\n","\n","    self.cls_token = nn.Parameter(torch.randn(1, 1, emb_size))\n","\n","  def forward(self, x: Tensor) -> Tensor:\n","    b, _, _, _ = x.shape\n","    x = self.proj(x)\n","    cls_tokens = repeat(self.cls_token, '() n e -> b n e', b = b)\n","    x = torch.cat([cls_tokens, x], dim = 1)\n","    return x\n","\n","PatchEmbedding()(X_train[0]).shape"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-05-09T19:32:00.28699Z","iopub.status.busy":"2022-05-09T19:32:00.285884Z","iopub.status.idle":"2022-05-09T19:32:00.299845Z","shell.execute_reply":"2022-05-09T19:32:00.298511Z","shell.execute_reply.started":"2022-05-09T19:32:00.286921Z"},"trusted":true},"outputs":[],"source":["class Net(nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","        self.conv1 = nn.Conv2d(3, 6, 5)\n","        self.pool = nn.MaxPool2d(2, 2)\n","        self.conv2 = nn.Conv2d(6, 16, 5)\n","        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n","        self.fc2 = nn.Linear(120, 84)\n","        self.fc3 = nn.Linear(84, 10)\n","\n","    def forward(self, x):\n","        x = self.pool(F.relu(self.conv1(x)))\n","        x = self.pool(F.relu(self.conv2(x)))\n","        x = torch.flatten(x, 1) # flatten all dimensions except batch\n","        x = F.relu(self.fc1(x))\n","        x = F.relu(self.fc2(x))\n","        x = self.fc3(x)\n","        return x\n","\n","net = Net()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-05-09T19:32:00.66177Z","iopub.status.busy":"2022-05-09T19:32:00.661479Z","iopub.status.idle":"2022-05-09T19:32:00.667807Z","shell.execute_reply":"2022-05-09T19:32:00.66651Z","shell.execute_reply.started":"2022-05-09T19:32:00.66174Z"},"trusted":true},"outputs":[],"source":["import torch.optim as optim\n","\n","criterion = nn.CrossEntropyLoss()\n","optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-05-09T19:32:52.901042Z","iopub.status.busy":"2022-05-09T19:32:52.899991Z","iopub.status.idle":"2022-05-09T19:32:52.940868Z","shell.execute_reply":"2022-05-09T19:32:52.939894Z","shell.execute_reply.started":"2022-05-09T19:32:52.900988Z"},"trusted":true},"outputs":[],"source":["for epoch in range(2):  # loop over the dataset multiple times\n","\n","    running_loss = 0.0\n","    for i, (inputs, labels) in enumerate(zip(X_train, y_train)):\n","        # zero the parameter gradients\n","        optimizer.zero_grad()\n","\n","        inputs = inputs.reshape((1, 1025, 216))\n","        outputs = net(inputs)\n","        loss = criterion(outputs, labels)\n","        loss.backward()\n","        optimizer.step()\n","\n","        # print statistics\n","        running_loss += loss.item()\n","        if i % 2000 == 1999:    # print every 2000 mini-batches\n","            print(f'[{epoch + 1}, {i + 1:5d}] loss: {running_loss / 2000:.3f}')\n","            running_loss = 0.0\n","\n","print('Finished Training')"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-04-28T15:54:58.849414Z","iopub.status.busy":"2022-04-28T15:54:58.848957Z","iopub.status.idle":"2022-04-28T15:54:58.859528Z","shell.execute_reply":"2022-04-28T15:54:58.85771Z","shell.execute_reply.started":"2022-04-28T15:54:58.849379Z"},"trusted":true},"outputs":[],"source":["f1score = f1_score(y_test, np.round(preds), average=\"micro\")\n","print(f1score)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-04-28T15:55:16.267799Z","iopub.status.busy":"2022-04-28T15:55:16.266911Z","iopub.status.idle":"2022-04-28T15:55:16.284361Z","shell.execute_reply":"2022-04-28T15:55:16.282635Z","shell.execute_reply.started":"2022-04-28T15:55:16.26775Z"},"trusted":true},"outputs":[],"source":["birdclef_dir = \"/kaggle/input/birdclef-2022/\"\n","\n","with open(birdclef_dir + \"scored_birds.json\") as fp:\n","    scored_birds = json.load(fp)\n","    \n","print(scored_birds)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-04-28T16:03:35.018776Z","iopub.status.busy":"2022-04-28T16:03:35.018055Z","iopub.status.idle":"2022-04-28T16:03:37.054201Z","shell.execute_reply":"2022-04-28T16:03:37.053318Z","shell.execute_reply.started":"2022-04-28T16:03:35.018735Z"},"trusted":true},"outputs":[],"source":["def eval_test_file(filedir, filename):\n","    data = []\n","    audio, sample_rate = librosa.load(filedir + filename)\n","    num_chunks = math.ceil(len(audio) / (sample_rate * 5))\n","    for chunk in range(num_chunks):\n","        starting_second = chunk * 5\n","        ending_second = (chunk + 1) * 5\n","        audio_chunk = audio[(sample_rate * starting_second):(sample_rate * ending_second)]\n","        pred = round(xg_reg.predict([audio_chunk])[0])\n","        bird_id = le.inverse_transform([pred])[0]\n","        for bird in scored_birds:\n","            row_id = filename[:-4] + \"_\" + bird + \"_\" + str(ending_second)\n","            result = bird == bird_id\n","            data.append((row_id, result))\n","    return data\n","\n","test_data_path = birdclef_dir + \"test_soundscapes/\"\n","out = eval_test_file(test_data_path, \"soundscape_453028782.ogg\")\n","out_df = pd.DataFrame(out, columns=[\"row_id\",\"result\"])\n","out_df.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-04-28T16:06:22.204909Z","iopub.status.busy":"2022-04-28T16:06:22.204167Z","iopub.status.idle":"2022-04-28T16:06:24.185703Z","shell.execute_reply":"2022-04-28T16:06:24.184872Z","shell.execute_reply.started":"2022-04-28T16:06:22.204868Z"},"trusted":true},"outputs":[],"source":["test_data_path = birdclef_dir + \"test_soundscapes/\"\n","filenames = sorted(os.listdir(test_data_path))\n","data = []\n","for filename in filenames:\n","    data += eval_test_file(test_data_path, filename)\n","test_df = pd.DataFrame(sorted(data), columns=['row_id', 'result'])\n","test_df.to_csv(\"submission.csv\", index=False)"]},{"cell_type":"markdown","metadata":{},"source":[]}],"metadata":{"interpreter":{"hash":"e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"},"kernelspec":{"display_name":"Python 3.8.10 64-bit","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.10"}},"nbformat":4,"nbformat_minor":4}
